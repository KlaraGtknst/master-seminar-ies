\begin{abstract}
    %The abstract should briefly summarize the contents of the paper in
    %150--250 words.
    % unsupervised learning
    Since labelled data is often scarce and expensive to obtain, 
    unsupervised learning has emerged as a powerful paradigm for training models without reliance on labelled data. 
    % SSL
    Within this domain, \acl{ssl} has gained significant attention, leveraging unlabelled data to generate labels.
    % CL
    A core concept within \acl{ssl} is \acl{cl}, which focuses on learning data representations by contrasting positive and negative sample pairs. 
    % representation learning
    The key idea is to encourage the model to map positive samples closer together in the representation space 
    while pushing negative samples farther apart.
    % hard positive/ negative pairs
    The selection of positive and negative pairs is of particular importance, 
    as challenging pairs present valuable learning opportunities for the model.
    % motivation
    A significant advantage of \acl{cl} in representation learning is its ability to capture the underlying structure of the data, 
    leading to the development of more robust and generalizable models.
    % this paper
    This review paper offers a comprehensive examination of various \acl{cl} approaches, 
    focusing on their foundational principles and the methods employed for pair selection, 
    while also providing a critical analysis of these techniques.
    
    \keywords{\acl{cl}  \and \acl{ssl} \and Hard sample mining.}
\end{abstract}