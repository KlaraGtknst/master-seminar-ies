The selection of positive pairs ($x$, $x^+$) is subject to multiple papers, 
which propose different strategies.
In the case of unsupervised learning, generally, 
the positive sample $x^+$ is generated by applying a augmentation to the anchor $x$.
Popular image augmentation techniques include random cropping, colour jittering, Gaussian blur,
rotation and scaling \citet{adversarial_2020,robinson_contrastive_2021,curricular_weighting_2024}.
Graph augmentations involve adding or removing edges and nodes, 
whereas words or sentences are added or masked in the context of natural language processing 
\citet{curricular_weighting_2024}.

% PU learning
Another approach is to use so-called \ac{pu} learning, where learning is carried out 
on a set of positive and a set of unlabelled samples \citet{chuang_debiased_2020}.
The challenge is to infer the presence of both positive and negative classes from this limited supervision, 
as no explicit negative samples are provided. 
\ac{pu} learning is applicable when obtaining negative labels is costly or impractical, 
but positive labels are available.