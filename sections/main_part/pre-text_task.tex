% SSL
\acf{ssl} is an unsupervised learning technique that enables the training of models on unlabelled data.
% pre-text task
This is accomplished through the implementation of pre-text tasks 
designed to create labels from the unlabelled dataset.
Researchers typically design these self-supervised pre-text tasks 
to facilitate label generation without the need for manual annotations \citep{PIC_2020}.
% Contrastive learning
One such pre-text task, \acf{cl}, 
focuses on learning data representations by contrasting positive and negative sample pairs. 
% positive and negative samples
In a scenario where a dataset consists of multiple (unlabelled) classes, 
instances from the same class are considered positive pairs, 
while instances from different classes are treated as negative pairs.
The key idea is to encourage the model to pull positive samples closer together 
in the representation space 
while pushing negative samples farther apart \citep{mining_potential_2024}.
