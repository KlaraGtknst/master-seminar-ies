% SSL
\acf{ssl} is an unsupervised learning technique that enables the training of models on unlabeled data.
% pre-text task
This is accomplished through the implementation of pre-text tasks 
designed to create labels from the unlabelled dataset.
Researchers typically design these self-supervised pre-text tasks 
to facilitate label generation without the need for manual annotations \citet{PIC_2020}.
% Contrastive learning
One such pre-text task, \acf{cl}, 
focuses on learning data representations by contrasting similar and dissimilar sample pairs. 
While similar pairs are also called positive pairs, 
dissimilar pairs are referred to as negative pairs.
The key idea is to encourage the model to pull similar samples closer together in the representation space 
while pushing dissimilar samples farther apart \citet{mining_potential_2024}.
% positive and negative samples
In a scenario where a dataset consists of multiple inherent (unlabelled) classes, 
instances from the same class are considered positive pairs, 
while instances from different classes are treated as negative pairs.
% goal of model
Consequently, the model learns to differentiate between instances and 
develops robustness to (image) transformations
\citet{PIC_2020,swav_2020,local_aggr_2019,grape_2024,CL_temp_2021}.