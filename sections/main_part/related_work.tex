\section{Related Work}\label{sec:related_work}

% sampling from distributions
Naturally, when using descriptive statistics to describe data, (empirical estimated) distributions play a crucial role.
Some researchers obtain their positive and negative samples from distributions.
Unfortunately, since \ac{cl} is a form of unsupervised learning, 
the true data distribution of the different classes is often not available.
Therefore, some scientists formulate assumptions or simplify the problem.
Possible assumptions include that the data distribution is uniform,
or approximating the positive sample distribution by sampling from a set of transformations 
or using the overall data distribution as a proxy for the negative sample distribution \citet{chuang_debiased_2020,robinson_contrastive_2021}.
In other cases, the class distributions are approximated using \acp{bmm} \citet{progcl_2022}.

Given the assumption that the data distribution sufficiently well approximated, 
it is possible to consider probabilities of samples being \acp{fn} during the selection process of samples.
Needless to say, the goal is to avoid sampling \acp{fn} as negative samples.
To this end, scientists have proposed different strategies, which mostly boil down to 
incorporating the possibility of a potential negative sample being a \ac{fn} or \ac{tn} 
\citet{chuang_debiased_2020,robinson_contrastive_2021,progcl_2022}.


% augmentation strategies
Irrespective of its usage in the context of estimation of distributions, 
data augmentation is a common technique to create positive samples.
Often, an augmentation strategy is randomly sampled from a set of possible augmentations.
The motivation behind this is to increase the diversity of the positive samples 
in order to drive the model to learn features invariant to translations 
\citet{PIC_2020,swav_2020,local_aggr_2019,grape_2024,CL_temp_2021}.


% clustering/ distance
Since \ac{cl} objectives are often formulated in terms of distances or similarities between pairs of samples, 
the idea of using clustering techniques is a natural choice.
Intuitively, clusters of similar samples should be considered as positive samples and thus,
should be encoded close to each other.
Conversely, samples from different clusters should be encoded far apart.

Multiple methods have been proposed to generate samples via clustering.
Some ideas focus on high intra-cluster similarities to improve the alignment of the embeddings \citet{DRC_2020}.
Other ideas define different neighbour regions to condense representation within an inner radius 
while repelling samples from an outer radius \citet{local_aggr_2019}.
Another approach is to consider both Euclidean distance and semantic similarity to generate hard samples \citet{mining_manifolds_2018}.
Moreover, the \ac{pcl} technique defines positive samples as cluster centroids 
from one of the multiple clusterings for different numbers of clusters 
to encode the hierarchical structure of the data \citet{PCL_2021}.


% memory bank
Another prominent concept is the usage of memory banks to store embeddings of the data.
\citeauthor{mochi_2020} fill these memory banks with embeddings of negative samples 
and propose two approaches for generating new hard negatives: 
Two of the most difficult samples currently stored in the memory bank are randomly selected and mixed.
The second approach is to use only one of the existing negative samples and 
mix it with the anchor to create a new sample.

\citet{progcl_2022} propose a method that extends the idea of \citet{mochi_2020} by weighting randomly selected negative samples 
with their relative similarity to the anchor when mixing them to create more difficult negative samples.

It is also possible to use the memory bank to store the embeddings of the positive samples.
Similarly, either randomly chosen samples can be used individually or 
the samples can be weighted by their hardness during loss calculation \citet{mining_potential_2024}.


% other work
% EM-algorithm
Multiple approaches, including \ac{drc}, \ac{pcl} and \progcl{}, use the \ac{em} algorithm 
to reduce computational costs or to find solutions via approximations.
Both \ac{drc} \citet{DRC_2020} and \progcl{} \citet{PCL_2021} are clustering-based methods 
while \progcl{} \citet{progcl_2022} is a distribution-based method.

% temperature
Since most approaches use a temperature parameter to control the hardness of the negative samples, 
\citet{CL_temp_2021} and \citet{grape_2024} investigate the impact of the temperature on the performance of the model.
They find that the \ac{cl} loss function optimizes hard samples by penalizing them according to their hardness.
If the temperature is small, only the closest points are penalized and others are not.
This can result in a uniformly distributed embedding space.

% curriculum learning
\citet{curricular_weighting_2024} propose a curriculum learning approach to generate hard negative samples.
They outline why curriculum learning is beneficial for \ac{cl} and how it can be implemented.

% data scheduling
% \citet{PIC_2020} propose a method to schedule the data for training 
% to reduce the periods within which a sample is not considered for training.