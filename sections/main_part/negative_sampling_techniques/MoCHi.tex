\subsection{\acl{mochi}}\label{subsec:MoCHi}

\citet{mochi_2020} claim that most approaches to sampling negatives rely on 
time-consuming updates of a memory bank or big batches to achieve good performance.
They propose \ac{mochi}, a method that computes samples online claiming without any computational overhead.

% memory bank
They use a memory bank $Q$ of size $| Q | = K$ to store negative samples.
There are different ways to choose $K$ by either saving the whole dataset, a queue of the last batches,
or all images in the current batch.

% definition of MoCHi
The definition of \ac{mochi}($N, s, s'$) is as follows:
\begin{enumerate}
    \item $N$: Number of negative samples from the memory bank to consider during negative mining.
    \item $s$: Number of samples to generate with approach 1.
    \item $s'$: Number of samples to generate with approach 2.
\end{enumerate}
For both approaches to generating negative samples, the authors use the same memory bank $Q$.
Given a fixed anchor, firstly, $Q$ is ordered in descending similarity to the anchor representation    
in the feature space producing $\tilde{Q}$.
The similarity between $n_j \in \tilde{Q}$ and the query $q$ is calculated via 
a scaled dot product of both samples.
Secondly, all but the first, i.e. most similar, $N$ samples are discarded from $\tilde{Q}$.
% first approach
The first approach proposed by \citeauthor{mochi_2020}, chooses two random samples $n_i, n_j \in \tilde{Q}$ 
and a mixing coefficient $\alpha_k \in (0,1)$, 
to generate the new sample as a linear combination of the existing hard negative samples 
as described in \eqref{eq:mochi_appr1}.

\begin{equation}
    h_k = \frac{\tilde{h_k}}{\left\| \tilde{h_k}  \right\|}; \tilde{h_k} = \alpha_k n_i + (1-\alpha_k)n_j
    \label{eq:mochi_appr1}
\end{equation}

% second approach
The second approach generates new samples by using the anchor representation $q$ 
and a random sample $n_j \in \tilde{Q}$ to generate the new sample as described in \eqref{eq:mochi_appr2}.
The mixing coefficient $\beta_k \in (0,0.5)$ is used to balance the influence of the anchor 
and the hard negative sample.
By limiting the influence of the anchor, the authors enforce a stronger influence of the negative sample.
This approach is considered to produce even harder negative samples than the first approach.

\begin{equation}
    h_k' = \frac{\tilde{h_k'}}{\left\| \tilde{h_k'}  \right\|}; \tilde{h_k'} = \beta q + (1-\beta_k)n_j
    \label{eq:mochi_appr2}
\end{equation}

% update of memory bank
Using \eqref{eq:mochi_appr1} and \eqref{eq:mochi_appr2}, 
$s$ and $s'$ hard negative samples are generated respectively.
Afterward, the similarity of each generated sample $h_k$ (or $h_k'$ respectively) 
to the anchor $q$ is calculated via $\frac{q^T h_k}{\tau}$.
This similarity is used to update the memory bank $\tilde{Q}$ 
by inserting the new sample.

% loss functions
There are two loss functions proposed by \citet{mochi_2020} to train the model.
Firstly, the alignment loss is used to determine 
the absolute distance between representations with the same class label.
Secondly, the uniformity loss is used to determine 
the distribution of representations on the hypersphere.
It is calculated via the logarithm of the average pairwise Gaussian potential between all embeddings.

% s, s' effect
\citet{mochi_2020} conducted experiments to investigate the effect of the parameters $s, s'$.
They present metrics such as average precision and accuracy for different ratios of $s$ and $s'$ 
for object detection and image segmentation tasks on COCO as well as 
linear classification on ImageNet-1K and object detection on PASCAL VOC.
Generally, the scores differ only in magnitudes of $1 \%$.
Additionally, the authors state that for $s > 0, s' = 0$ the model learns faster but achieves similar performance to the baseline, i.e. the model without \ac{mochi}.


% modifications
The authors listed modifications to \ac{mochi} that lead to inferior performance.
They tried to define the mixing coefficient via the similarity of the hard negative samples 
to the anchor. % how negative the sample is 
Moreover, they introduced non-uniformity when sampling $n_i, n_j$ from $\tilde{Q}$ by defining a 
probability distribution over similarities to the query. % too hard samples
Alternatively, they proposed omitting the parameters $s, s'$ and 
sampling hard negatives using alternately approaches 1 and 2 until $N\%$ of the top samples in 
$\tilde{Q}$ correspond to synthesized samples.

% scheme 2 from ProGCL: ProGCL-mix (MoCHi)
\begin{equation}
    \tilde{h_k} =  \alpha_k \cdot n_i + (1-\alpha_k) \cdot  n_j, \text{ where } \alpha_k = \frac{p(n_i \text{ is } TP)}{p(n_i \text{ is } TP) + p(n_j \text{ is } TP)}
    \label{eq:progcl_mix}
\end{equation}

The researches that introduced \progcl{} (\autoref{subsec:graph_distribution}) in \citet{progcl_2022} 
present an extension of the hard mining strategy \ac{mochi}.% introduced in \autoref{subsec:MoCHi}.
They claim, that a high portion of the negative samples generated by \ac{mochi} are \acp{fn}.
They therefore propose to weigh the selected negative samples for mixing 
according to their relative probability of being a \ac{tn} as displayed in \eqref{eq:progcl_mix}.