\subsection{\acl{mochi}}\label{subsec:MoCHi}

\citet{mochi_2020} state that most approaches to sampling negatives rely on 
time-consuming updates of a memory bank or big batches to achieve good performance.
They propose \ac{mochi}, a method that computes samples online while claiming they introduce no computational overhead.

% memory bank
They use a memory bank $Q$ of size $| Q | = K$ to store negative samples.
There are different ways to choose $K$ by either saving the whole dataset, a queue of last batches,
or all images in the current batch.

% definition of MoCHi
The definition of \ac{mochi}($N, s, s'$) is as follows:
\begin{enumerate}
    \item $N$: Number of negative samples from the memory bank to consider during negative mining.
    \item $s$: Number of samples to generate with approach 1.
    \item $s'$: Number of samples to generate with approach 2.
\end{enumerate}
For both approaches to generating negative samples, the authors use the same memory bank $Q$.
Given a fixed anchor, firstly, $Q$ is ordered in descending similarity to the anchor representation 
in the feature space producing $\tilde{Q}$.
Secondly, all but the first, i.e. most similar, $N$ samples are discarded from $\tilde{Q}$.
% first approach
The first approach proposed by \citeauthor{mochi_2020}, chooses two random samples $n_i, n_j \in \tilde{Q}$ 
and a mixing coefficient $\alpha_k \in (0,1)$, 
to generate the new sample as a linear combination of the existing hard negative samples 
as described in \eqref{eq:mochi_appr1}.

\begin{equation}
    h_k = \frac{\tilde{h_k}}{\left\| \tilde{h_k}  \right\|}; \tilde{h_k} = \alpha_k n_i + (1-\alpha_k)n_j
    \label{eq:mochi_appr1}
\end{equation}

% second approach
The second approach generates new samples by using the anchor representation $q$ 
and a random sample $n_j \in \tilde{Q}$ to generate the new sample as described in \eqref{eq:mochi_appr2}.
The mixing coefficient $\beta_k \in (0,0.5)$ is used to balance the influence of the anchor 
and the hard negative sample.
By limiting the influence of the anchor, the authors enforce a stronger influence of the negative sample.
This approach is considered to produce even harder negative samples than the first approach.

\begin{equation}
    h_k' = \frac{\tilde{h_k'}}{\left\| \tilde{h_k'}  \right\|}; \tilde{h_k'} = \beta q + (1-\beta_k)n_j
    \label{eq:mochi_appr2}
\end{equation}

% update of memory bank
Using \eqref{eq:mochi_appr1} and \eqref{eq:mochi_appr2}, 
$s$ and $s'$ hard negative samples are generated respectively.
Afterward, the similarity of the generated sample $h_k$ (or $h_k'$ respectively) 
to the anchor $q$ is calculated via $\frac{q^T h_k}{\tau}$.
This similarity is used to update the memory bank $\tilde{Q}$ 
by inserting the new sample.

% loss functions
There are two loss functions proposed by \citet{mochi_2020} to train the model.
Firstly, the alignment loss is used to determine 
the absolute distance between representations with the same class label.
Secondly, the uniformity loss is used to determine 
the distribution of representations on the hyperphere.
It is calculated via the logarithm of the average pairwise Gaussian potential between all embeddings.

% modifications
The authors listed modifications to the approach proposed above that lead to inferior performance.
They tried to define the mixing coefficient via the similarity of the hard negative samples 
to the anchor. % how negative the sample is 
Moreover, they introduced non-uniformity when sampling $n_i, n_j$ from $\tilde{Q}$ by defining a 
probability distribution over similarities to the query. % too hard samples
Alternatively, they proposed omitting the parameters $s, s'$ and 
sampling hard negatives using alternately approaches 1 and 2 until $N\%$ of the top samples in 
$\tilde{Q}$ correspond to synthesized samples.