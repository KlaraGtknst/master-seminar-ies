
@misc{robinson_contrastive_2021,
	title = {Contrastive {Learning} with {Hard} {Negative} {Samples}},
	url = {http://arxiv.org/abs/2010.04592},
	abstract = {How can you sample good negative examples for contrastive learning? We argue that, as with metric learning, contrastive learning of representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). The key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use true similarity information. In response, we develop a new family of unsupervised sampling methods for selecting hard negative samples where the user can control the hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The proposed method improves downstream performance across multiple modalities, requires only few additional lines of code to implement, and introduces no computational overhead.},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Robinson, Joshua and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie},
	month = jan,
	year = {2021},
	note = {arXiv:2010.04592 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2021},
	file = {Contrastive_Learning_with_hard_negative_samples.pdf:/Users/klara/Documents/uni/Master/SoSe24/IES_Seminar/Contrastive_Learning_with_hard_negative_samples.pdf:application/pdf},
}

@misc{mochi_2020,
	title = {Hard {Negative} {Mixing} for {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2010.01028},
	abstract = {Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both crucial in learning such representations. At the same time, data mixing strategies either at the image or the feature level improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, i.e., the effect of hard negatives, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing the memory size, however, leads to diminishing returns in terms of performance. We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing, we propose hard negative mixing strategies at the feature level, that can be computed on-the-fly with a minimal computational overhead. We exhaustively ablate our approach on linear classification, object detection and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Kalantidis, Yannis and Sariyildiz, Mert Bulent and Pion, Noe and Weinzaepfel, Philippe and Larlus, Diane},
	month = dec,
	year = {2020},
	note = {arXiv:2010.01028 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted at NeurIPS 2020. Project page with pretrained models: https://europe.naverlabs.com/mochi},
	file = {Hard_negavte_mixing_for_contrastive_learning.pdf:/Users/klara/Documents/uni/Master/SoSe24/IES_Seminar/Hard_negavte_mixing_for_contrastive_learning.pdf:application/pdf},
}

@inproceedings{chuang_debiased_2020,
	title = {Debiased {Contrastive} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chuang, Ching-Yao and Robinson, Joshua and Lin, Yen-Chen and Torralba, Antonio and Jegelka, Stefanie},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {8765--8775},
}

@inproceedings{PIC_2020,
	title = {Parametric {Instance} {Classification} for {Unsupervised} {Visual} {Feature} learning},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b427426b8acd2c2e53827970f2c2f526-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cao, Yue and Xie, Zhenda and Liu, Bin and Lin, Yutong and Zhang, Zheng and Hu, Han},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {15614--15624},
}

@inproceedings{ho_contrastive_2020,
	title = {Contrastive {Learning} with {Adversarial} {Examples}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/c68c9c8258ea7d85472dd6fd0015f047-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Chih-Hui and Nvasconcelos, Nuno},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {17081--17093},
}

@misc{li_prototypical_2021,
	title = {Prototypical {Contrastive} {Learning} of {Unsupervised} {Representations}},
	url = {http://arxiv.org/abs/2005.04966},
	abstract = {This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that addresses the fundamental limitations of instance-wise contrastive learning. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Li, Junnan and Zhou, Pan and Xiong, Caiming and Hoi, Steven C. H.},
	month = mar,
	year = {2021},
	note = {arXiv:2005.04966 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Li et al. - 2021 - Prototypical Contrastive Learning of Unsupervised .pdf:/Users/klara/Zotero/storage/P55EGCTR/Li et al. - 2021 - Prototypical Contrastive Learning of Unsupervised .pdf:application/pdf},
}

@article{swav_2020,
  title={Unsupervised learning of visual features by contrasting cluster assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9912--9924},
  year={2020}
}

@article{DRC_2020,
  title={Deep robust clustering by contrastive learning},
  author={Zhong, Huasong and Chen, Chong and Jin, Zhongming and Hua, Xian-Sheng},
  journal={arXiv preprint arXiv:2008.03030},
  year={2020}
}

@inproceedings{local_aggr_2019,
  title={Local aggregation for unsupervised learning of visual embeddings},
  author={Zhuang, Chengxu and Zhai, Alex Lin and Yamins, Daniel},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6002--6012},
  year={2019}
}


@inproceedings{mining_manifolds_2018,
  title={Mining on manifolds: Metric learning without labels},
  author={Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\v{r}}ej},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7642--7651},
  year={2018}
}

@inproceedings{grape_2024,
	address = {New York, NY, USA},
	series = {{WWW} '24},
	title = {Towards {Expansive} and {Adaptive} {Hard} {Negative} {Mining}: {Graph} {Contrastive} {Learning} via {Subspace} {Preserving}},
	isbn = {9798400701719},
	shorttitle = {Towards {Expansive} and {Adaptive} {Hard} {Negative} {Mining}},
	url = {https://doi.org/10.1145/3589334.3645327},
	doi = {10.1145/3589334.3645327},
	abstract = {Graph Neural Networks (GNNs) have emerged as the predominant approach for analyzing graph data on the web and beyond. Contrastive learning (CL), a self-supervised paradigm, not only mitigates reliance on annotations but also has potential in performance. The hard negative sampling strategy that benefits CL in other domains proves ineffective in the context of Graph Contrastive Learning (GCL) due to the message passing mechanism. Embracing the subspace hypothesis in clustering, we propose a method towards expansive and adaptive hard negative mining, referred to as G raph contR astive leA rning via subsP ace prE serving (GRAPE ). Beyond homophily, we argue that false negatives are prevalent over an expansive range and exploring them confers benefits upon GCL. Diverging from existing neighbor-based methods, our method seeks to mine long-range hard negatives throughout subspace, where message passing is conceived as interactions between subspaces. \%Empirical investigations back up this strategy. Additionally, our method adaptively scales the hard negatives set through subspace preservation during training. In practice, we develop two schemes to enhance GCL that are pluggable into existing GCL frameworks. The underlying mechanisms are analyzed and the connections to related methods are investigated. Comprehensive experiments demonstrate that our method outperforms across diverse graph datasets and remains competitive across varied application scenarios{\textbackslash}footnoteOur code is available at https://github.com/zz-haooo/WWW24-GRAPE. .},
	urldate = {2024-07-22},
	booktitle = {Proceedings of the {ACM} on {Web} {Conference} 2024},
	publisher = {Association for Computing Machinery},
	author = {Hao, Zhezheng and Xin, Haonan and Wei, Long and Tang, Liaoyuan and Wang, Rong and Nie, Feiping},
	month = may,
	year = {2024},
	pages = {322--333},
}


@misc{progcl_2022,
	title = {{ProGCL}: {Rethinking} {Hard} {Negative} {Mining} in {Graph} {Contrastive} {Learning}},
	shorttitle = {{ProGCL}},
	url = {http://arxiv.org/abs/2110.02027},
	doi = {10.48550/arXiv.2110.02027},
	abstract = {Contrastive Learning (CL) has emerged as a dominant technique for unsupervised representation learning which embeds augmented versions of the anchor close to each other (positive samples) and pushes the embeddings of other samples (negatives) apart. As revealed in recent studies, CL can benefit from hard negatives (negatives that are most similar to the anchor). However, we observe limited benefits when we adopt existing hard negative mining techniques of other domains in Graph Contrastive Learning (GCL). We perform both experimental and theoretical analysis on this phenomenon and find it can be attributed to the message passing of Graph Neural Networks (GNNs). Unlike CL in other domains, most hard negatives are potentially false negatives (negatives that share the same class with the anchor) if they are selected merely according to the similarities between anchor and themselves, which will undesirably push away the samples of the same class. To remedy this deficiency, we propose an effective method, dubbed {\textbackslash}textbf\{ProGCL\}, to estimate the probability of a negative being true one, which constitutes a more suitable measure for negatives' hardness together with similarity. Additionally, we devise two schemes (i.e., {\textbackslash}textbf\{ProGCL-weight\} and {\textbackslash}textbf\{ProGCL-mix\}) to boost the performance of GCL. Extensive experiments demonstrate that ProGCL brings notable and consistent improvements over base GCL methods and yields multiple state-of-the-art results on several unsupervised benchmarks or even exceeds the performance of supervised ones. Also, ProGCL is readily pluggable into various negatives-based GCL methods for performance improvement. We release the code at {\textbackslash}textcolor\{magenta\}\{{\textbackslash}url\{https://github.com/junxia97/ProGCL\}\}.},
	urldate = {2024-07-22},
	publisher = {arXiv},
	author = {Xia, Jun and Wu, Lirong and Wang, Ge and Chen, Jintao and Li, Stan Z.},
	month = jun,
	year = {2022},
	note = {arXiv:2110.02027 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Accetpted at ICML 2022},
	file = {arXiv Fulltext PDF:/Users/klara/Zotero/storage/N5NGGYLY/Xia et al. - 2022 - ProGCL Rethinking Hard Negative Mining in Graph C.pdf:application/pdf;arXiv.org Snapshot:/Users/klara/Zotero/storage/5EVIPNK9/2110.html:text/html},
}

@article{zhuang_mining_2024,
	title = {Mining negative samples on contrastive learning via curricular weighting strategy},
	volume = {668},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S002002552400447X},
	doi = {10.1016/j.ins.2024.120534},
	abstract = {Contrastive learning, which pulls positive pairs closer and pushes away negative pairs, has remarkably propelled the development of self-supervised representation learning. Previous studies either neglected negative sample selection, resulting in suboptimal performance, or emphasized hard negative samples from the beginning of training, potentially leading to convergence issues. Drawing inspiration from curriculum learning, we find that learning with negative samples ranging from easy to hard improves both model performance and convergence rate. Therefore, we propose a dynamic negative sample weighting strategy for contrastive learning. Specifically, we design a loss function that adaptively adjusts the weights assigned to negative samples based on the model's performance. Initially, the loss prioritizes easy samples, but as training advances, it shifts focus to hard samples, enabling the model to learn more discriminative representations. Furthermore, to prevent an undue emphasis on false negative samples during later stages, which probably results in trivial solutions, we apply L2 regularization on the weights of hard negative samples. Extensive qualitative and quantitative experiments demonstrate the effectiveness of the proposed weighting strategy. The ablation study confirms both the reasonableness of the curriculum and the effectiveness of the regularization.},
	urldate = {2024-07-22},
	journal = {Information Sciences},
	author = {Zhuang, Jin and Jing, Xiao-Yuan and Jia, Xiaodong},
	month = may,
	year = {2024},
	keywords = {Adaptive curriculum learning, Contrastive learning, Hard sample mining, Negative sampling},
	pages = {120534},
}

@inproceedings{wang_understanding_2021,
	title = {Understanding the {Behaviour} of {Contrastive} {Loss}},
	url = {https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Understanding_the_Behaviour_of_Contrastive_Loss_CVPR_2021_paper.html},
	language = {en},
	urldate = {2024-07-23},
	author = {Wang, Feng and Liu, Huaping},
	year = {2021},
	pages = {2495--2504},
	file = {Full Text PDF:/Users/klara/Zotero/storage/DB6KD3TC/Wang und Liu - 2021 - Understanding the Behaviour of Contrastive Loss.pdf:application/pdf},
}
