
@misc{robinson_contrastive_2021,
	title = {Contrastive {Learning} with {Hard} {Negative} {Samples}},
	url = {http://arxiv.org/abs/2010.04592},
	abstract = {How can you sample good negative examples for contrastive learning? We argue that, as with metric learning, contrastive learning of representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). The key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use true similarity information. In response, we develop a new family of unsupervised sampling methods for selecting hard negative samples where the user can control the hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The proposed method improves downstream performance across multiple modalities, requires only few additional lines of code to implement, and introduces no computational overhead.},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Robinson, Joshua and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie},
	month = jan,
	year = {2021},
	note = {arXiv:2010.04592 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2021},
	file = {Contrastive_Learning_with_hard_negative_samples.pdf:/Users/klara/Documents/uni/Master/SoSe24/IES_Seminar/Contrastive_Learning_with_hard_negative_samples.pdf:application/pdf},
}

@misc{mochi_2020,
	title = {Hard {Negative} {Mixing} for {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2010.01028},
	abstract = {Contrastive learning has become a key component of self-supervised learning approaches for computer vision. By learning to embed two augmented versions of the same image close to each other and to push the embeddings of different images apart, one can train highly transferable visual representations. As revealed by recent studies, heavy data augmentation and large sets of negatives are both crucial in learning such representations. At the same time, data mixing strategies either at the image or the feature level improve both supervised and semi-supervised learning by synthesizing novel examples, forcing networks to learn more robust features. In this paper, we argue that an important aspect of contrastive learning, i.e., the effect of hard negatives, has so far been neglected. To get more meaningful negative samples, current top contrastive self-supervised learning approaches either substantially increase the batch sizes, or keep very large memory banks; increasing the memory size, however, leads to diminishing returns in terms of performance. We therefore start by delving deeper into a top-performing framework and show evidence that harder negatives are needed to facilitate better and faster learning. Based on these observations, and motivated by the success of data mixing, we propose hard negative mixing strategies at the feature level, that can be computed on-the-fly with a minimal computational overhead. We exhaustively ablate our approach on linear classification, object detection and instance segmentation and show that employing our hard negative mixing procedure improves the quality of visual representations learned by a state-of-the-art self-supervised learning method.},
	urldate = {2024-06-14},
	publisher = {arXiv},
	author = {Kalantidis, Yannis and Sariyildiz, Mert Bulent and Pion, Noe and Weinzaepfel, Philippe and Larlus, Diane},
	month = dec,
	year = {2020},
	note = {arXiv:2010.01028 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted at NeurIPS 2020. Project page with pretrained models: https://europe.naverlabs.com/mochi},
	file = {Hard_negavte_mixing_for_contrastive_learning.pdf:/Users/klara/Documents/uni/Master/SoSe24/IES_Seminar/Hard_negavte_mixing_for_contrastive_learning.pdf:application/pdf},
}

@inproceedings{chuang_debiased_2020,
	title = {Debiased {Contrastive} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/63c3ddcc7b23daa1e42dc41f9a44a873-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chuang, Ching-Yao and Robinson, Joshua and Lin, Yen-Chen and Torralba, Antonio and Jegelka, Stefanie},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {8765--8775},
}

@inproceedings{PIC_2020,
	title = {Parametric {Instance} {Classification} for {Unsupervised} {Visual} {Feature} learning},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b427426b8acd2c2e53827970f2c2f526-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Cao, Yue and Xie, Zhenda and Liu, Bin and Lin, Yutong and Zhang, Zheng and Hu, Han},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {15614--15624},
}

@inproceedings{ho_contrastive_2020,
	title = {Contrastive {Learning} with {Adversarial} {Examples}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/c68c9c8258ea7d85472dd6fd0015f047-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Chih-Hui and Nvasconcelos, Nuno},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {17081--17093},
}

@misc{li_prototypical_2021,
	title = {Prototypical {Contrastive} {Learning} of {Unsupervised} {Representations}},
	url = {http://arxiv.org/abs/2005.04966},
	abstract = {This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that addresses the fundamental limitations of instance-wise contrastive learning. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.},
	urldate = {2024-06-20},
	publisher = {arXiv},
	author = {Li, Junnan and Zhou, Pan and Xiong, Caiming and Hoi, Steven C. H.},
	month = mar,
	year = {2021},
	note = {arXiv:2005.04966 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Li et al. - 2021 - Prototypical Contrastive Learning of Unsupervised .pdf:/Users/klara/Zotero/storage/P55EGCTR/Li et al. - 2021 - Prototypical Contrastive Learning of Unsupervised .pdf:application/pdf},
}

@article{caron2020unsupervised,
  title={Unsupervised learning of visual features by contrasting cluster assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9912--9924},
  year={2020}
}

@article{DRC_2020,
  title={Deep robust clustering by contrastive learning},
  author={Zhong, Huasong and Chen, Chong and Jin, Zhongming and Hua, Xian-Sheng},
  journal={arXiv preprint arXiv:2008.03030},
  year={2020}
}

@inproceedings{local_aggr_2019,
  title={Local aggregation for unsupervised learning of visual embeddings},
  author={Zhuang, Chengxu and Zhai, Alex Lin and Yamins, Daniel},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6002--6012},
  year={2019}
}


@inproceedings{mining_manifolds_2018,
  title={Mining on manifolds: Metric learning without labels},
  author={Iscen, Ahmet and Tolias, Giorgos and Avrithis, Yannis and Chum, Ond{\v{r}}ej},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7642--7651},
  year={2018}
}